{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIV-VH5XThkM"
   },
   "source": [
    "# Home Assignment #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ehsan Shaghaei\n",
    "> Nov 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tD4mewy2wVo8"
   },
   "source": [
    "Furter, PCA is used for dimensionality reduction, so **it is prohibited** to have `n_components` equal to `n_features`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkUamcwmTxJj"
   },
   "source": [
    "## Task 1. PCA drawbacks (3 points)\n",
    "\n",
    "You are given a classification task. You wish to use PCA for dimensionality reduction to get rid of noise in data and capture the most important information. After you use Principal Component Analysis, you realize that classification accuracy has severly decreased. \n",
    "\n",
    "Describe three possible scenarios when first components of PCA remove not noise but information useful for classification. **(0.5 pts * 3)**  \n",
    "1. Independent variables become less interpretable:\n",
    "    After implementing PCA on the dataset, your original features will turn into Principal Components. Principal Components are the linear combination of your original features. Principal Components are not as readable and interpretable as original features.\n",
    "2. Data standardization is must before PCA: \n",
    "    You must standardize your data before implementing PCA, otherwise PCA will not be able to find the optimal Principal Components. For instance, if a feature set has data expressed in units of Kilograms, Light years, or Millions, the variance scale is huge in the training set. If PCA is applied on such a feature set, the resultant loadings for features with high variance will also be large. Hence, principal components will be biased towards features with high variance, leading to false results. Also, for standardization, all the categorical features are required to be converted into numerical features before PCA can be applied. PCA is affected by scale, so you need to scale the features in your data before applying PCA. Use StandardScaler from Scikit Learn to standardize the dataset features onto unit scale (mean = 0 and standard deviation = 1) which is a requirement for the optimal performance of many Machine Learning algorithms\n",
    "3. Information Loss: Although Principal Components try to cover maximum variance among the features in a dataset, if we donâ€™t select the number of Principal Components with care, it may miss some information as compared to the original list of features.\n",
    "\n",
    "\n",
    "Re-create the described scenarios, display the data before and after the PCA proving your assumption. **(0.5 pts * 3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kixx38eOTvKc"
   },
   "outputs": [],
   "source": [
    "# write your code \n",
    "# case 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILLjCC1-YMm7"
   },
   "source": [
    "## Task 2. Is there any difference? (2 points)\n",
    "\n",
    "In some cases, sequential application of PCA and Logistic Regression (LR) leads to the very same separating hyperplane as just application LR (let us denote it as $PCA + LR \\sim LR$). In such cases there is a specific relation **R** between the projection hyperplane of PCA and separating hyperplane of LR. \n",
    "\n",
    "What is the relation **R**? Describe it and explain why **R** is necessary and sufficient for $PCA + LR \\sim LR$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Bkn4o6MaWcr"
   },
   "source": [
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ow7HYiKFa8Ay"
   },
   "source": [
    "## Task 3. PCA, LDA and QDA (1 point)\n",
    "\n",
    "Provide an example when both PCA followed by LDA and LDA alone work worse than QDA. Provide a theoretical explanation of the proposed case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tAWA3cYbNwg"
   },
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmcuxAlzbPkb"
   },
   "source": [
    "## Task 4. PCA, LDA, QDA and...PCA (4 points)\n",
    "\n",
    "Provide an example when both PCA followed by LDA and LDA alone have accuracy on the valid set < 100%, whereas QDA has accuracy on valid set = 100% and:\n",
    "1. PCA followed by QDA has accuracy on valid set < 100% (1 pt)\n",
    "2. PCA followed by QDA has accuracy on valid set = 100% (3 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KjQdzXrkbW8X"
   },
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
