{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3PeyY81JLMY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML on Edge\n",
    "## Ehsna Shaghaei\n",
    "Nov 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EB4zCjyoJ3Md",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML on Edge\n",
    "## Ehsan Shaghaei\n",
    "Nov 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnDRvYpvJ-zA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "most of the time the so called “smart” devices are programmed to act like remote controlled devices controlled either by cloud or an app or just stream the sensor readings to the cloud where the actual processing happens . Given the limited RAM or the processing power available on these resource constraint devices there are only limited things that can be accomplished ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KO0IFJ8cJWiW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "![image.png](https://yastatic.net/s3/lpc/d875ecb7-7b11-4d77-95c8-adb76f5a1895.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpDnvwyqKU7e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Platforms\n",
    "\n",
    "Initially, in 2019 TF announced support of microcontrollers, We have been hearing AI on edge as being the logical next step in the evolution of IOT devices but given the lack of open source frameworks there was very less innovation in this direction and with Google’s announcement it has opened lot of doors for embedded programmers to try build AI applications on edge ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Z0fRVTGK4zK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "currently, There is  edge-ml platform  which is an open-source and browser-based toolchain for machine learning on microcontrollers.\n",
    "![](https://edge-ml.org/images/process.svg)\n",
    "It supports ml-flow and with a few simple steps edge-ml lets you record data, label samples, train models and deploy validated embedded machine learning directly on the edge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQza6HylM4R5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Play around\n",
    "I had few ESP32 Cam modules lying around and There were this guy on linkedIn deploying different models on a MUC,so I thought why not train and deploy a Fashion Mnist model to recognize fashion apparels directly using the onboard camera feed . The outcome beat my expectation , the application was able to recognize the images with reasonable accuracy .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tddqpZCQmUKv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **Building TFlite model for Fashion Mnist dataset**\n",
    "This notebook uses the Fashion MNIST dataset which contains 70,000 grayscale images in 10 categories. Each image in the dataset is a grayscale image of 28 x 28 pixels ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xG9ccGRRfyi0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DCAFonp4f9ZP",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MNU1D-FgDzQ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TQjyNN7ugCLT",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSDQ7W8xgYwt",
    "outputId": "53a98ece-13f9-4059-a0f9-faa473a4d456",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset 29.45 MiB (download: 29.45 MiB, generated: 36.42 MiB, total: 65.87 MiB) to ~/tensorflow_datasets/fashion_mnist/3.0.1...\n",
      "Dataset fashion_mnist downloaded and prepared to ~/tensorflow_datasets/fashion_mnist/3.0.1. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "splits, info = tfds.load('fashion_mnist', with_info=True, as_supervised=True, \n",
    "                         split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'])\n",
    "\n",
    "(train_examples, validation_examples, test_examples) = splits\n",
    "\n",
    "num_examples = info.splits['train'].num_examples\n",
    "num_classes = info.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ItQhlzglgZtF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt_top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aKIUbY_ngqNA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Store the labels in a text file to be downloaded later \n",
    "with open('labels.txt', 'w') as f:\n",
    "  f.write('\\n'.join(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MJh71WXXhlkN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# this will be our input size\n",
    "IMG_SIZE = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbjParUZkAUC",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NoYEfpwbj9Yf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def format_example(image, label):\n",
    "  image = tf.cast(image, tf.float16)\n",
    "  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "  image = image / 255.0\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8HMGI0i4kJDd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idQqnFMOkPF5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create a Dataset from images and labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "F5CT9a4ckVi-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_batches = train_examples.cache().shuffle(num_examples//4).batch(BATCH_SIZE).map(format_example).prefetch(1)\n",
    "validation_batches = validation_examples.cache().batch(BATCH_SIZE).map(format_example).prefetch(1)\n",
    "test_batches = test_examples.cache().batch(1).map(format_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xJ8eELdkkuI",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building and Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "n0M4w9HLke8H",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Conv2D(6, 3, activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(6, 3, activation='relu'),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R_He1Fo2ku05",
    "outputId": "00c02f4c-26f8-445e-b21c-ca75f4db906f",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1715/1715 [==============================] - 18s 5ms/step - loss: 0.5703 - accuracy: 0.7985 - val_loss: 0.4204 - val_accuracy: 0.8483\n",
      "Epoch 2/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.4111 - accuracy: 0.8556 - val_loss: 0.3935 - val_accuracy: 0.8578\n",
      "Epoch 3/40\n",
      "1715/1715 [==============================] - 6s 3ms/step - loss: 0.3769 - accuracy: 0.8671 - val_loss: 0.3677 - val_accuracy: 0.8680\n",
      "Epoch 4/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.3500 - accuracy: 0.8751 - val_loss: 0.3281 - val_accuracy: 0.8820\n",
      "Epoch 5/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.3312 - accuracy: 0.8834 - val_loss: 0.3737 - val_accuracy: 0.8607\n",
      "Epoch 6/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.3177 - accuracy: 0.8867 - val_loss: 0.3192 - val_accuracy: 0.8843\n",
      "Epoch 7/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.3048 - accuracy: 0.8911 - val_loss: 0.3236 - val_accuracy: 0.8830\n",
      "Epoch 8/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2944 - accuracy: 0.8952 - val_loss: 0.3005 - val_accuracy: 0.8888\n",
      "Epoch 9/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2862 - accuracy: 0.8978 - val_loss: 0.3092 - val_accuracy: 0.8902\n",
      "Epoch 10/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2802 - accuracy: 0.8990 - val_loss: 0.3039 - val_accuracy: 0.8928\n",
      "Epoch 11/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2741 - accuracy: 0.9020 - val_loss: 0.2878 - val_accuracy: 0.8960\n",
      "Epoch 12/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2672 - accuracy: 0.9043 - val_loss: 0.2883 - val_accuracy: 0.8965\n",
      "Epoch 13/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2631 - accuracy: 0.9053 - val_loss: 0.3010 - val_accuracy: 0.8928\n",
      "Epoch 14/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2579 - accuracy: 0.9075 - val_loss: 0.2970 - val_accuracy: 0.8962\n",
      "Epoch 15/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2542 - accuracy: 0.9083 - val_loss: 0.2797 - val_accuracy: 0.9003\n",
      "Epoch 16/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2510 - accuracy: 0.9093 - val_loss: 0.2884 - val_accuracy: 0.8955\n",
      "Epoch 17/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2478 - accuracy: 0.9113 - val_loss: 0.2918 - val_accuracy: 0.8975\n",
      "Epoch 18/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2455 - accuracy: 0.9108 - val_loss: 0.2868 - val_accuracy: 0.8982\n",
      "Epoch 19/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2430 - accuracy: 0.9117 - val_loss: 0.2849 - val_accuracy: 0.8985\n",
      "Epoch 20/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2389 - accuracy: 0.9141 - val_loss: 0.2870 - val_accuracy: 0.8992\n",
      "Epoch 21/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2362 - accuracy: 0.9149 - val_loss: 0.2962 - val_accuracy: 0.8977\n",
      "Epoch 22/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2341 - accuracy: 0.9161 - val_loss: 0.2814 - val_accuracy: 0.9043\n",
      "Epoch 23/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2330 - accuracy: 0.9147 - val_loss: 0.2879 - val_accuracy: 0.9002\n",
      "Epoch 24/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2302 - accuracy: 0.9165 - val_loss: 0.2864 - val_accuracy: 0.9017\n",
      "Epoch 25/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2283 - accuracy: 0.9168 - val_loss: 0.3006 - val_accuracy: 0.8962\n",
      "Epoch 26/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2260 - accuracy: 0.9184 - val_loss: 0.2910 - val_accuracy: 0.8985\n",
      "Epoch 27/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2248 - accuracy: 0.9184 - val_loss: 0.2883 - val_accuracy: 0.9002\n",
      "Epoch 28/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2227 - accuracy: 0.9198 - val_loss: 0.2951 - val_accuracy: 0.9007\n",
      "Epoch 29/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2216 - accuracy: 0.9189 - val_loss: 0.3160 - val_accuracy: 0.8927\n",
      "Epoch 30/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2204 - accuracy: 0.9203 - val_loss: 0.2990 - val_accuracy: 0.8997\n",
      "Epoch 31/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2189 - accuracy: 0.9190 - val_loss: 0.2955 - val_accuracy: 0.8983\n",
      "Epoch 32/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2164 - accuracy: 0.9207 - val_loss: 0.2886 - val_accuracy: 0.9040\n",
      "Epoch 33/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2169 - accuracy: 0.9218 - val_loss: 0.2956 - val_accuracy: 0.9002\n",
      "Epoch 34/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2158 - accuracy: 0.9211 - val_loss: 0.2978 - val_accuracy: 0.9018\n",
      "Epoch 35/40\n",
      "1715/1715 [==============================] - 7s 4ms/step - loss: 0.2147 - accuracy: 0.9216 - val_loss: 0.2970 - val_accuracy: 0.8982\n",
      "Epoch 36/40\n",
      "1715/1715 [==============================] - 7s 4ms/step - loss: 0.2109 - accuracy: 0.9235 - val_loss: 0.3031 - val_accuracy: 0.8980\n",
      "Epoch 37/40\n",
      "1715/1715 [==============================] - 6s 4ms/step - loss: 0.2101 - accuracy: 0.9231 - val_loss: 0.2983 - val_accuracy: 0.9017\n",
      "Epoch 38/40\n",
      "1715/1715 [==============================] - 7s 4ms/step - loss: 0.2102 - accuracy: 0.9231 - val_loss: 0.2926 - val_accuracy: 0.9050\n",
      "Epoch 39/40\n",
      "1715/1715 [==============================] - 7s 4ms/step - loss: 0.2085 - accuracy: 0.9235 - val_loss: 0.3080 - val_accuracy: 0.8953\n",
      "Epoch 40/40\n",
      "1715/1715 [==============================] - 7s 4ms/step - loss: 0.2084 - accuracy: 0.9244 - val_loss: 0.2957 - val_accuracy: 0.9035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc6604ac590>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_batches, \n",
    "          epochs=40,\n",
    "          validation_data=validation_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiQtqatAk-sb",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exporting to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3D0eAQ3plAnt",
    "outputId": "c0e2f8c6-2cb1-4ddc-942e-8dab8153ff82",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "export_dir = 'saved_model/1'\n",
    "tf.saved_model.save(model, export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4JN13mz7lMXV",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the model.\n",
    "def representative_data_gen():\n",
    "  for input_value, _ in test_batches.take(100):\n",
    "    yield [input_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "V7PJxINqlNDZ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "r_kwrs7clRFI",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tflite_model_file = 'model.tflite'\n",
    "\n",
    "with open(tflite_model_file, \"wb\") as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crymzcQXl6T1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Ju-V0RMel5lQ",
    "outputId": "22fe73d2-1de9-4f85-aa6d-5041fdd607a4",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_5a8763ae-cbf6-493e-8938-38c2e0692902\", \"model.tflite\", 11520)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_94b78b17-6ddb-4a9c-8492-a4b23bf37545\", \"labels.txt\", 75)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "\n",
    "  files.download(tflite_model_file)\n",
    "  files.download('labels.txt')\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6KHTzWHQIwH",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deployment to Edge medium\n",
    "The edge medium limitations can make the deployment stage challenging, limitations might be:\n",
    "- there is no support of a filesystem \n",
    "- low resources e.g. little memory, little processing unit\n",
    "\n",
    "Because of little ram we export our model values to a C source file.\n",
    "\n",
    "``` xxd -i model.tflite > model_data.cc```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS2xANL4Sv1b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To deploy this model we use **tfmicro** library  \n",
    "which is a TensorFlow lite interpreter developed by TFLite team which will interpret our model and get us predictions . We add these two components under “components” directory as shown above .\n",
    "\n",
    "The hardware I used for the demo is AI thinker’s ESP CAM module ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjX1kHfWTVKu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The next step is to place the “model_data.cc” file we built in the last step of “Building the model” in “main/tf_model/” folder . Make sure that the variable names of the model array and array length in “include/model_data.h” are same as in “model_data.cc” file . Next we check the “/include/model_settings.h” file to make sure that the settings such as input size and number of categories match the model that we are deploying, if you are using any other models you need to modify the settings to match your model.\n",
    "\n",
    "The setup process for tfmicro library is simple ,\n",
    "\n",
    "First, we map the model_data using the GetModel function and pass the model data array name as the argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBSZ5in8TmB8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```\n",
    "model = tflite::GetModel(model_data_tflite);\n",
    "```\n",
    "Second , we pull in the operation resolver which contains operations needed to realize the model. Here I used “AllOpsResolver” which includes all operations , best practice would be to include only the operations needed for your model and hence save some code space.\n",
    "```\n",
    "static tflite::ops::micro::AllOpsResolver micro_op_resolver;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SFKNNTHT8uO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we make the model interpretter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9FixdK_UEqC",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```\n",
    "// Build an interpreter to run the model with.\n",
    "static tflite::MicroInterpreter static_interpreter(\n",
    "model, micro_op_resolver, tensor_arena, kTensorArenaSize, error_reporter);\n",
    "interpreter = &static_interpreter;\n",
    "\n",
    "// Allocate memory from the tensor_arena for the model's tensors.\n",
    "TfLiteStatus allocate_status = interpreter->AllocateTensors();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6MZ8td1UPd8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This completes the setup process, we are now ready to start interpreting the input data to get our predictions. In order to infer the data we need to first fill interpreter’s input buffer with our input data and then call interpreter’s “invoke” function to run inference , the prediction are stored in interpreter’s output buffer ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntv8EgPCUb8G",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "``` \n",
    "// fetches the input buffer of the interperter where we fill our input data\n",
    "interpreter->input(0)\n",
    "\n",
    "// Invoking the function to run the model in the input data\n",
    "interpreter->Invoke()\n",
    "\n",
    "// Fetch the outcome of inference using the output buffer\n",
    "interpreter->output(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fpehxx3vNNoA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://miro.medium.com/max/720/1*do6x-6rJdK-uqaWgT8mj8A.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXKeSwGKVOUD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "[Repo](https://github.com/akshayvernekar/)\n",
    "\n",
    "[tflite](https://www.tensorflow.org/lite)\n",
    "\n",
    "[edge-ml](https://edge-ml.org/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
